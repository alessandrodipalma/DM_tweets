{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utilities import to_float, get_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = get_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94974,
     "status": "ok",
     "timestamp": 1665240486261,
     "user": {
      "displayName": "ALESSANDRO DIPALMA",
      "userId": "15827404338341614451"
     },
     "user_tz": -120
    },
    "id": "UTjV9Hkmc7OI",
    "outputId": "44aa4fcc-7776-4a21-f8e0-db83f7f8c316"
   },
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(DATA_PATH + 'tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv(DATA_PATH + 'users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1665240486264,
     "user": {
      "displayName": "ALESSANDRO DIPALMA",
      "userId": "15827404338341614451"
     },
     "user_tz": -120
    },
    "id": "NP_Jn2WOd21-",
    "outputId": "a0dc8edf-24d7-4ecc-8d60-c6f001f6f0ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           id              name lang  bot           created_at  statuses_count\n0  2353593986   Lamonica Raborn   en    1  2019-02-22 18:00:42            76.0\n1  2358850842     Lourie Botton   en    0  2019-02-26 03:02:32            54.0\n2   137959629  Dadan Syarifudin   en    1  2015-04-30 07:09:56             3.0\n3   466124818    Carletto Focia   it    1  2017-01-18 02:49:18            50.0\n4  2571493866         MBK Ebook   en    0  2019-06-18 19:30:21          7085.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>lang</th>\n      <th>bot</th>\n      <th>created_at</th>\n      <th>statuses_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2353593986</td>\n      <td>Lamonica Raborn</td>\n      <td>en</td>\n      <td>1</td>\n      <td>2019-02-22 18:00:42</td>\n      <td>76.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2358850842</td>\n      <td>Lourie Botton</td>\n      <td>en</td>\n      <td>0</td>\n      <td>2019-02-26 03:02:32</td>\n      <td>54.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>137959629</td>\n      <td>Dadan Syarifudin</td>\n      <td>en</td>\n      <td>1</td>\n      <td>2015-04-30 07:09:56</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>466124818</td>\n      <td>Carletto Focia</td>\n      <td>it</td>\n      <td>1</td>\n      <td>2017-01-18 02:49:18</td>\n      <td>50.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2571493866</td>\n      <td>MBK Ebook</td>\n      <td>en</td>\n      <td>0</td>\n      <td>2019-06-18 19:30:21</td>\n      <td>7085.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1665240486266,
     "user": {
      "displayName": "ALESSANDRO DIPALMA",
      "userId": "15827404338341614451"
     },
     "user_tz": -120
    },
    "id": "4z_xApMQfk0A",
    "outputId": "e95fdcef-1ca4-4e15-b719-9a0c7b80de86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11508 entries, 0 to 11507\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              11508 non-null  int64  \n",
      " 1   name            11507 non-null  object \n",
      " 2   lang            11508 non-null  object \n",
      " 3   bot             11508 non-null  int64  \n",
      " 4   created_at      11508 non-null  object \n",
      " 5   statuses_count  11109 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 539.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_users.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any user record has a null value in it.\n",
    "\n",
    "Extract the only record with a null name (id = 1535)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1665240486267,
     "user": {
      "displayName": "ALESSANDRO DIPALMA",
      "userId": "15827404338341614451"
     },
     "user_tz": -120
    },
    "id": "2zF4wF2vfmmH",
    "outputId": "a796ccb3-662f-40a2-c945-36436fccb572"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              id name lang  bot           created_at  statuses_count\n1535  2166124159  NaN   en    0  2018-11-02 06:39:14          6566.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>lang</th>\n      <th>bot</th>\n      <th>created_at</th>\n      <th>statuses_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1535</th>\n      <td>2166124159</td>\n      <td>NaN</td>\n      <td>en</td>\n      <td>0</td>\n      <td>2018-11-02 06:39:14</td>\n      <td>6566.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_null = df_users.isnull()\n",
    "idx_null = df_users.index[df_null[\"name\"] == True].tolist()\n",
    "df_users.iloc[idx_null]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6960,
     "status": "ok",
     "timestamp": 1665240493189,
     "user": {
      "displayName": "ALESSANDRO DIPALMA",
      "userId": "15827404338341614451"
     },
     "user_tz": -120
    },
    "id": "bZfdvzMejsEe",
    "outputId": "2358873b-fa51-4ca4-e0ed-6b065be11546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13664696 entries, 0 to 13664695\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count     Dtype \n",
      "---  ------          --------------     ----- \n",
      " 0   id              13664694 non-null  object\n",
      " 1   user_id         13447413 non-null  object\n",
      " 2   retweet_count   13227562 non-null  object\n",
      " 3   reply_count     13016818 non-null  object\n",
      " 4   favorite_count  13017154 non-null  object\n",
      " 5   num_hashtags    12607172 non-null  object\n",
      " 6   num_urls        13016073 non-null  object\n",
      " 7   num_mentions    12810531 non-null  object\n",
      " 8   created_at      13664696 non-null  object\n",
      " 9   text            13126975 non-null  object\n",
      "dtypes: object(10)\n",
      "memory usage: 1.0+ GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\al\\AppData\\Local\\Temp\\ipykernel_10368\\3419106329.py:1: FutureWarning: null_counts is deprecated. Use show_counts instead\n",
      "  df_tweets.info(null_counts=True)\n"
     ]
    }
   ],
   "source": [
    "df_tweets.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1665240493189,
     "user": {
      "displayName": "ALESSANDRO DIPALMA",
      "userId": "15827404338341614451"
     },
     "user_tz": -120
    },
    "id": "AtL8X7Furq0Z",
    "outputId": "1b51e155-6f83-43ca-d51a-0cb5b952f5c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "136646960"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of tweets, including possible duplicates\n",
    "df_tweets.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicate records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 2 million tweets are exact duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66058,
     "status": "ok",
     "timestamp": 1665240559240,
     "user": {
      "displayName": "ALESSANDRO DIPALMA",
      "userId": "15827404338341614451"
     },
     "user_tz": -120
    },
    "id": "-3ywwV6CoQ1V",
    "outputId": "e3daccc7-e3b6-4dfb-86ae-1f0bf9101419"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "False    11712597\nTrue      1952099\ndtype: int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Null user_id and text\n",
    "We remove the records that have invalid user_id and text, since these records can't be used neither to analyze user behaviour, neither to perform any kind of topic analysis on the text. \n",
    "\n",
    "The only utility that they can have could be related to the twitter density for a given period of time, but we go for the deletion since 56280 is not a great amount of records w.r.t. the total number of tweets. Also, notice that the counters have several null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.drop(df_tweets.index[df_tweets.user_id.isnull() & df_tweets.text.isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.to_csv(path_or_buf=DATA_PATH + 'tweets_no_dupl.csv', sep='#', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(DATA_PATH + 'tweets_no_dupl.csv', sep='#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Drop duplicates over all columns\n",
    "First, we drop the duplicates by all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.drop_duplicates(keep='first')\n",
    "df_tweets.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.to_csv(path_or_buf=DATA_PATH + 'tweets_no_dupl_1.1.csv', sep='#', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(DATA_PATH + 'tweets_no_dupl_1.1.csv', sep='#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for duplicates on a subset of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID\n",
    "The are a considerable amount of tweets with same id, but we can't consider them duplicates since the id has no semantic that we can check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.duplicated(subset=['id']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.duplicated(subset=['id', 'user_id']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.duplicated(subset=['id', 'user_id', 'text']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.duplicated(subset=['id', 'user_id', 'created_at', 'text']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 ['user_id', 'text','created_at']\n",
    "Considering the triple  `['user_id', 'text','created_at']`, we can see that about 10% of the data consists in duplicated records and we are not able to detect them if we include the `id` value in the duplicate definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Criterion for duplicates\n",
    "duplicates_bool = df_tweets.duplicated(subset=['user_id', 'text', 'created_at'], keep=False)\n",
    "df_tweets[duplicates_bool].info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the other columns of these duplicates have a considerable amount of null values. \n",
    "\n",
    "In order to keep the correct values, we proceed by performing a merge of the copies, keeping the non null value (usually the max value).\n",
    "\n",
    "We first convert the counts and nums to numeric type, setting to -1 the meaningless values.\n",
    "Then, the merge will proceed by taking the max over the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for attr in ['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'user_id']:\n",
    "    df_tweets[attr + '_conv'] = df_tweets[attr].apply(to_float)\n",
    "\n",
    "# recreate the index column for the groupby\n",
    "df_tweets = df_tweets.reset_index()\n",
    "df_tweets.index = df_tweets['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aggregated = df_tweets[duplicates_bool].groupby(['user_id', 'text', 'created_at']).agg(\n",
    "    {'retweet_count_conv': max, 'reply_count_conv': max, 'favorite_count_conv': max,\n",
    "     'num_hashtags_conv': max, 'num_urls_conv': max, 'num_mentions_conv': max,\n",
    "     'id': tuple, 'index': min}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aggregated.index = aggregated['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "attributes = ['retweet_count_conv', 'reply_count_conv', 'favorite_count_conv', 'num_hashtags_conv', 'num_mentions_conv',\n",
    "              'num_urls_conv']\n",
    "\n",
    "# Saves index and the attributes\n",
    "df_tweets.loc[aggregated.index, attributes] = aggregated[attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# reset the -1 to NaN to check how many null values have been removed\n",
    "for a in attributes:\n",
    "    df_tweets[a] = df_tweets[a].replace(-1, np.NaN)\n",
    "\n",
    "df_tweets[duplicates_bool].info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.drop_duplicates(['user_id', 'text', 'created_at'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check that the non nan copies have been kept\n",
    "df_tweets[df_tweets.index.isin(aggregated.index)].info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems fine, so we rename the 'conv' columns to the original name and drop the uncleaned ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dropped = df_tweets.drop(\n",
    "    columns=['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = dropped.rename(columns={'retweet_count_conv': 'retweet_count',\n",
    "                                    'reply_count_conv': 'reply_count',\n",
    "                                    'favorite_count_conv': 'favorite_count',\n",
    "                                    'num_hashtags_conv': 'num_hashtags',\n",
    "                                    'num_urls_conv': 'num_urls',\n",
    "                                    'num_mentions_conv': 'num_mentions'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.reset_index()\n",
    "df_tweets = df_tweets.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.to_csv(path_or_buf=DATA_PATH + 'tweets_no_dupl_user_text_createdat.csv', sep='#', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(DATA_PATH + 'tweets_no_dupl_user_text_createdat.csv', sep='#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how many duplicates we have for each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no records where both the user_id and text fields are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets[df_tweets.user_id.isnull() & df_tweets.text.isnull()].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 [text, created_at]\n",
    "We now consider the situation where `text` and `created_at` are equal among different records. \n",
    "\n",
    "Despite it is certainly possible that two users post the same text at the same time, the situation is suspicious, and we check the possibility that the two records are the same tweet from the same user, but with mispelled `user_id`.\n",
    "\n",
    "For this analysis we consider only the tweets with non-null text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Criterion for duplication\n",
    "duplicates_bool = ~df_tweets.text.isnull() & df_tweets.duplicated(subset=['text', 'created_at'], keep=False)\n",
    "duplicates_bool.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets[duplicates_bool & (df_tweets.user_id_conv == -1)].info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a NaN or alphanumeric `user_id` to be wrong, since the correct format is the one with only numerical characters.\n",
    "Out of the 646625 duplicates by `[text,created_at]`, 321006 have wrong `user_id`.\n",
    "We proceed by keeping the copy with the right `user_id`, and selecting the counter attributes by a max operation, as we did at step **1.2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for a in ['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions']:\n",
    "    df_tweets[a] = df_tweets[a].replace(np.NaN, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# recreate the index column for the groupby\n",
    "df_tweets = df_tweets.reset_index()\n",
    "df_tweets.index = df_tweets['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def select_best_user_id(user_ids):\n",
    "    greater_than_zero = []\n",
    "    smaller_than_zero = []\n",
    "    for id in user_ids:\n",
    "        if id > 0:\n",
    "            greater_than_zero.append(id)\n",
    "        else:\n",
    "            smaller_than_zero.append(id)\n",
    "\n",
    "    # if all but one are < 0, keep the one that is > 0\n",
    "    if len(greater_than_zero) == 1:\n",
    "        return greater_than_zero[0]\n",
    "    if len(greater_than_zero) > 1:\n",
    "        return tuple(greater_than_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aggregated = df_tweets[duplicates_bool].groupby(['text', 'created_at']).agg(\n",
    "    {'retweet_count': max, 'reply_count': max, 'favorite_count': max,\n",
    "     'num_hashtags': max, 'num_urls': max, 'num_mentions': max,\n",
    "     'user_id_conv': select_best_user_id,\n",
    "     'index': min}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aggregated.index = aggregated['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Count how many duplicates were from the same user but with misspelled user_id (== True)\n",
    "tweets_with_valid_user_id = aggregated.user_id_conv.apply(type) == float\n",
    "tweets_with_valid_user_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aggregated_one_text = aggregated[tweets_with_valid_user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "attributes = ['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_mentions', 'num_urls',\n",
    "              'user_id_conv']\n",
    "df_tweets.loc[aggregated_one_text.index, attributes] = aggregated_one_text[attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates which fit both criteria of duplication and misspelled user_id\n",
    "indices_to_drop = duplicates_bool & ~df_tweets.index.isin(aggregated.index)\n",
    "indices_to_drop.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[~indices_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df_tweets = df_tweets.drop(columns=['index'])\n",
    "df_tweets = df_tweets.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.to_csv(path_or_buf=DATA_PATH + 'tweets_no_dupl_1.3.csv', sep='#', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(DATA_PATH + 'tweets_no_dupl_1.3.csv', sep='#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional checks of duplicates\n",
    "\n",
    "We check other combinations of attributes to find duplicates.\n",
    "\n",
    "Most of the combinations don't have a significant number of values which could be considered duplicates, apart from the ['user_id', 'text'] combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for attr in ['id', ['id', 'user_id'], ['id', 'created_at'], ['id', 'user_id', 'created_at'], ['user_id', 'text'],\n",
    "             ['id', 'user_id', 'text']]:\n",
    "    counts = df_tweets.duplicated(attr).value_counts()\n",
    "    if len(counts) > 1:\n",
    "        dupl_count = counts[1]\n",
    "    else:\n",
    "        dupl_count = 0\n",
    "    print(f\"{attr} {dupl_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 [user_id,text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicates by ['user_id', 'text'] are more than one million, which is a considerable amount and is worth further investigation. \n",
    "\n",
    "It's surely possible that a user, especially if it is a bot, tweets many times the same text. What we want to check is just that among these duplicates all the dates are valid, which is, in a range that goes from the Twitter foundation up to september 2022.\n",
    "\n",
    "If the dates are valid, we keep the tweets. Otherwise it is considered a noisy duplicate and is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "twitter_foundation = pd.to_datetime([\"20060321\"]).astype(np.int64)[0]\n",
    "sep_2022 = pd.to_datetime([\"20220915\"]).astype(np.int64)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def wrong_date(date):\n",
    "    return (date < twitter_foundation) | (date > sep_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Cast dates to seconds\n",
    "df_tweets['created_at_conv'] = pd.to_datetime(df_tweets['created_at']).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Criterion for duplicates\n",
    "duplicates_bool = df_tweets.duplicated(['user_id', 'text'], keep=False)\n",
    "duplicates_bool.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Replace incorrect values with NaN to check how many they are\n",
    "for attr in ['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_mentions', 'num_urls']:\n",
    "    df_tweets[attr].replace(-1, np.NaN)\n",
    "\n",
    "# All records which are potential duplicates and have a wrong date \n",
    "df_tweets[duplicates_bool][wrong_date(df_tweets.created_at_conv)].info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not aggregate the counters using a max operator as in previous steps, since the multiple \"copies\" in this case could be just a periodic tweet. Furthermore, there is no null value in the counters.\n",
    "Given that there is no null value in the other fields, we will simply remove the records having a wrong date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets[duplicates_bool].created_at_conv.apply(wrong_date).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these duplicates 97602 records have wrong created_at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "indices_to_drop = ~(duplicates_bool & df_tweets[duplicates_bool].created_at_conv.apply(wrong_date))\n",
    "indices_to_drop.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[indices_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.drop(columns='index').reset_index()\n",
    "df_tweets.to_csv(path_or_buf=DATA_PATH + 'tweets_no_dupl_1.4.csv', sep='#', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(DATA_PATH + 'tweets_no_dupl_1.4.csv', sep='#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 [user_id, created_at]\n",
    "\n",
    "Considering additional combinations of attributes, the most relevant number of duplicates regards the ['user_id', 'created_at'] combination of attributes (['user_id', 'text'] was already analyzed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for attr in ['id', ['id', 'user_id'], ['id', 'created_at'], ['user_id', 'created_at'], ['created_at', 'text'],\n",
    "             ['id', 'user_id', 'created_at'], ['user_id', 'text'], ['id', 'user_id', 'text']]:\n",
    "    counts = df_tweets.duplicated(attr).value_counts()\n",
    "    if len(counts) > 1:\n",
    "        dupl_count = counts[1]\n",
    "    else:\n",
    "        dupl_count = 0\n",
    "    print(f\"{attr} {dupl_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Criterion to select duplicates, considering non-null values\n",
    "duplicates_bool = df_tweets.duplicated(['user_id', 'created_at'], keep=False) & ~df_tweets.user_id.isnull()\n",
    "duplicates_bool.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicates that have non-null text also have all non-null counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets[duplicates_bool & ~df_tweets.text.isnull()].info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Duplicates with null text\n",
    "df_tweets[duplicates_bool & df_tweets.text.isnull()].info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicates removal procedeed with the merge procedure as in 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Replace null values with -1\n",
    "for a in ['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions']:\n",
    "    df_tweets[a] = df_tweets[a].replace(np.NaN, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "For duplicates it checks if the texts of the different records are:\n",
    "- if only one is non-null it outputs this text\n",
    "- all NaN it outputs NaN\n",
    "- if all are non-null returns the list of all texts\n",
    "'''\n",
    "def select_non_null_text(texts):\n",
    "    nonnull_texts = []\n",
    "    for text in texts:\n",
    "        if ~pd.Series(text).isnull()[0]:\n",
    "            nonnull_texts.append(text)\n",
    "    if len(nonnull_texts) == 1:\n",
    "        return nonnull_texts[0]\n",
    "    elif len(nonnull_texts) == 0:\n",
    "        return np.NaN\n",
    "    elif len(nonnull_texts) > 1:\n",
    "        return nonnull_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df_tweets = df_tweets.drop(columns='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "groupby_userid_createdat = df_tweets[duplicates_bool].groupby(['user_id', 'created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Save indices of duplicated tweets in a list for each record\n",
    "aggregated = groupby_userid_createdat.agg(\n",
    "    retweet_count=('retweet_count', max),\n",
    "    reply_count=('reply_count', max),\n",
    "    favorite_count=('favorite_count', max),\n",
    "    num_hashtags=('num_hashtags', max),\n",
    "    num_urls=('num_urls', max),\n",
    "    num_mentions=('num_mentions', max),\n",
    "    text=('text', select_non_null_text),\n",
    "    Keep_index=('index', min),\n",
    "    all_indices=('index', list)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Indices to keep\n",
    "aggregated.index = aggregated['Keep_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Records which have as text_type a string were the ones which had a single non-null text,\n",
    "while the others contain the list of texts for the duplicate records\n",
    "'''\n",
    "aggregated['text_type'] = aggregated.text.apply(type)\n",
    "aggregated.text_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# All records which had only one clean tweet, and the other ones were noisy\n",
    "aggregated_one_text = aggregated[aggregated.text_type == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aggregated[aggregated.text_type == list].all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Tweets which have multiple non-null texts are to keep because we can't determine if they are noisy\n",
    "not_replaceable = np.concatenate(aggregated[aggregated.text_type == list].all_indices.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "attributes = ['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_mentions', 'num_urls', 'text']\n",
    "df_tweets.loc[aggregated_one_text.index, attributes] = aggregated_one_text[attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Select all records which are the noisy counterparts of records with only one clean text\n",
    "or\n",
    "all records which are replaceable\n",
    "'''\n",
    "indices_to_keep = duplicates_bool.index.isin(aggregated_one_text.index) | duplicates_bool.index.isin(not_replaceable) | ~duplicates_bool\n",
    "# duplicates_bool.index.isin(aggregated_one_text.index) | duplicates_bool.index.isin(not_replaceable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Selects only the records to keep\n",
    "df_tweets = df_tweets[indices_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.reset_index()\n",
    "df_tweets = df_tweets.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.to_csv(path_or_buf=DATA_PATH + 'tweets_no_dupl_1.5.csv', sep='#', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(DATA_PATH + 'tweets_no_dupl_1.5.csv', sep='#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for attr in ['id', ['id', 'user_id'], ['id', 'created_at'], ['id', 'text'], ['id', 'user_id', 'text'], ['id','user_id','created_at'], \\\n",
    "            ['user_id', 'created_at'], ['user_id', 'text'], ['created_at', 'text']]:\n",
    "    counts = df_tweets.duplicated(attr).value_counts()\n",
    "    if len(counts) > 1:\n",
    "        dupl_count = counts[1]\n",
    "    else:\n",
    "        dupl_count = 0\n",
    "    print(f\"{attr} {dupl_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_tweets[df_tweets.duplicated(['id', 'user_id', 'text'], keep=False)].info()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
